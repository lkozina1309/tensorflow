{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN2o+sLZuX1++0C0ym7D9lq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkozina1309/tensorflow/blob/main/CNN/Jupyter/data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script *data_augmentation.py* adds data_augmentation to classification of dogs and cats to bring more accuracy to the model. "
      ],
      "metadata": {
        "id": "6Grcj9yuFT9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have to import the libraries."
      ],
      "metadata": {
        "id": "VQpz4fXyKRZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "LXUsWiLNKWuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we have to download the dataset.\n"
      ],
      "metadata": {
        "id": "kolq8KhzLIlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE = (32, 32)\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
        "                                                            shuffle=True,\n",
        "                                                            batch_size=BATCH_SIZE,\n",
        "                                                            image_size=IMG_SIZE)"
      ],
      "metadata": {
        "id": "Nq1k-U7YLO_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check our classes."
      ],
      "metadata": {
        "id": "e4NxBhdILSKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_dataset.class_names\n",
        "\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "_uP8NWDsMBPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll use validation split and create test set as original dataset doesn't contain it."
      ],
      "metadata": {
        "id": "G2SxZAREMtzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
        "                                                                 shuffle=True,\n",
        "                                                                 batch_size=BATCH_SIZE,\n",
        "                                                                 image_size=IMG_SIZE)"
      ],
      "metadata": {
        "id": "fvRQlROAOXfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the dataset."
      ],
      "metadata": {
        "id": "hCBRf47GgK8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "wIXC-fORgNzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "test_dataset = validation_dataset.take(val_batches // 5)\n",
        "validation_dataset = validation_dataset.skip(val_batches // 5)"
      ],
      "metadata": {
        "id": "zM5rA1YWOY8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use data rescaling as well as random flipping and rotation."
      ],
      "metadata": {
        "id": "gycGzSG9w6aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rescale = tf.keras.Sequential([\n",
        "  layers.Rescaling(1./255)\n",
        ")\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "mGP2cVUyybQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model is going to have 3 convolutional layers and 2 max_pooling layers beetween and we'll add dropout to avoid overfitting."
      ],
      "metadata": {
        "id": "JIyZSzYmsBi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "resize_and_rescale,\n",
        "data_augmentation,\n",
        "layers.Conv2D(16, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "layers.MaxPooling2D(2, 2),\n",
        "layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "layers.MaxPooling2D(2, 2),\n",
        "layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "layers.Dropout(0.4),\n",
        "layers.Flatten(),\n",
        "layers.Dense(128, activation='relu'),\n",
        "layers.Dense(2),\n",
        "])"
      ],
      "metadata": {
        "id": "28D9fE1wtVTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use \"adam\" as an optimizer and \"SparseCategoricalCrossentropy\" as a loss function and we'll run it through 10 epochs."
      ],
      "metadata": {
        "id": "UFAXisfitlgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=30, \n",
        "                    validation_data=validation_dataset)"
      ],
      "metadata": {
        "id": "hnyB4JEzvrK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should get accuracy of more than 70%."
      ],
      "metadata": {
        "id": "H8l_Zq6Gv7vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "KP24hy33v18S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}